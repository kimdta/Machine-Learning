{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to the seventh lab. After learning about SVMs last week, we finally introduce the _kernel trick_ and make them capable of tackling nonlinear data. We also introduced more generally the concept of _function mapping_ and learned a bit about word embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Function Mappings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 **[3pt]** Give an _original_ example for each of the following concepts:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Mapping from an example data type to a decision space\n",
    "2. Inverse mapping\n",
    "3. Mapping from the example data to two destination feature spaces\n",
    "4. Mapping from the two feature spaces above to a decision space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Given a picture, decide if it represents an apple or an orange. Original space: \n",
    "$64\\times64\\times3$ images; destination space: $\\{$apple, orange$\\}$.\n",
    "2. Given the label apple, map it to a $64\\times64\\times3$ picture of the fruit.\n",
    "3. (Feature Extraction) Map $\\Phi_1$ goes from the picture to an estimate of average color; map $\\Phi_2$ goes from the picture to a fruit width measured in pixels.\n",
    "4. (Decision Mapping) Map $\\Phi_3$ goes from the two features of estimated color and fruit width, to the decision space of classifying the fruit as apple or orange."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 **[1pt]** Explain advantages and disadvantages of Bag Of Words versus Word Embedding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Bag of Words (BoW), the feature vectors are very high dimensional (size of the dictionary) and sparse (each text only uses a subset of the dictionary) while there's smaller and dense feature space in Word Embedding (WE).\n",
    "\n",
    "More importantly, BoW only uses tallying of the used words (counts the number of occurences) while WE calculate similarities between words as vector distances, encoding words that tends to appear often together with closer vectors in feature space. This allows deduction and reasoning of word relationships based on vector similarity (distance and angle)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 **[2pt]** Refer to the graph exemplifying Word Embedding in the slides, and its explanation. (i) What does it mean that the point representing Paris is close to the point representing Berlin? (ii) Why is the point for Paris closer to the point for France than to the point representing Italy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(i) The point representing Paris is close to the point representing Berlin: these two words refer to similar concepts that are commonly found together in the text corpus.\n",
    "\n",
    "(ii) The point for Paris closer to the point for France than to the point representing Italy: Paris tends to appear more often closer to France than to Italy in the text corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Kernels theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 **[1pt]** Write the definition of kernel function (use latex)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$k(x,y) = \\langle \\phi(x), \\phi(y) \\rangle$    $\\forall x,y \\in X$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 **[1pt]** Explain the Representer property in English."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each datapoint $x$ in **original space** is represented by a real valued function $\\phi(x)= k(.,x)$ in **feature space** $\\mathcal{H}$. In another words, each $x \\in X$ is associatedd with its kernelized counterpart $k(.,x) \\in \\mathcal{H}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 **[2pt]** Explain the kernel trick in English."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The kernel function equals to the inner product of mapped data:\n",
    "$k(x,y) = \\langle \\phi(x), \\phi(y) \\rangle  = \\langle k(.,x), k(.,y) \\rangle$.\n",
    "\n",
    "**Kernel trick** states that the kernel functon between 2 points in oringinal space is equivalent to the **inner product between the mapping of the 2 data points** into RKHS. This is useful because it allows us to use all the positive properties of RKHS while avoiding explicitly define the mapping function, which can be potentially complex and timeconsuming (kernel trick).\n",
    "\n",
    "The actual formulation of the kernel function can be any form that produces a Gram matrix that is symmetric and positive semi-definite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 **[1pt]** Explain in English the required properties of a Mercer kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Symmetry: kernel function is unchanged if we switch the order of the parameters.\n",
    "* Positive definiteness: The Gram matrix of the kernel is positive semi- definite for any finite subset of the input space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5 **[1pt]** Calculate by hand the linear kernel on points $\\{[2,4], [1, -2]\\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The linear kernel is computed as: \n",
    "\n",
    "$k(x_1,x_2) = x_1^Tx_2$\n",
    "\n",
    "$k([2,4],[1,-2]) = [2,4]^T.[1,-2] = 2*1+4*(-2) = -6$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.6 **[1pt]** How do you compute the entry of the Gram matrix for row $i$ and column $j$ for a Gaussian kernel?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"radial\" Gaussian kernel:\n",
    "\n",
    "$K_{ij}= k(x_i,x_j) = exp(-\\gamma. \\|x_i - x_j\\|^2)$ for $\\gamma >0$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.7 **[2pt]** Explain why does the Perceptron work with non-linearly separable data using Kernelization. Do you think Linear Regression would work with Kernelization? Explain your reasoning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we know Perceptron never stops in non-linearly separable data. However Kernelization's able to map non-linear separation data to a Hilbert space where the data is hopefully linearly separable. Thus, Kernel Perceptron is able to do linear separation.\n",
    "\n",
    "The same works with any linear algorithm, including Linear Regression: once Kernelization takes care of nonlinearities, the linear algorithm can do its job without problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.8 **[1pt]** Explain the Universal Consistency of SVMs (in English)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVMs with Gaussian kernels are *universal function approximators*, meaning that they can approximate any arbitrary function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Kernels in practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity, let's use once again a two-species adaptation of the Iris dataset. You can copy the code from the last assignment. This time though, to make it harder for linear classifiers let's separate the \"central\" species from the other two. This means that you should set label `versicolor` rather than `setosa` as class `-1`. I suggest you un-comment the `pairplot`s to verify it works.  \n",
    "NOTE: all recommendation on how to handle and prepare the data from the past assignment(s) still hold. As do the warnings that using the wrong data sets will **invalidate the whole answer**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "sns.set(rc={'figure.figsize':(8,6)}, style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = sns.load_dataset('iris')\n",
    "#sns.pairplot(df, hue='species')\n",
    "\n",
    "df.loc[df['species'] == 'versicolor', 'species'] = -1\n",
    "df.loc[df['species'] != -1, 'species'] = 1\n",
    "df['species'] = pd.to_numeric(df['species'])\n",
    "print(df.dtypes)\n",
    "#sns.pairplot(df, hue='species')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data:\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(df, test_size= 0.2) #80-20 split\n",
    "\n",
    "#sns.pairplot(test, hue='species')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 **[1pt]** Train an SVM with linear kernel on the Iris data using Scikit-learn (this time you are required to use class `SVC`). Then do the same using a Gaussian kernel (still `SVC`) and compare the performance using its method `score()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Remember to prepare inputs/labels for Scikit-learn; again the last assignment should help.\n",
    "- Calling the method `score()` on the trained model just does the prediction and returns the percentage of correct answers. It is a useful function to learn to quickly check if your model is working.\n",
    "- You expect the linear kernel to perform poorly. If the performance is close to the Gaussian kernel, it is possible that the test set was by chance not homogeneous. You can verify that by doing a pairplot on the test set, and if so just run the data loading and preparation again.\n",
    "- No need to find an optimal value for `C` but pass it explicitly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7\n",
      "0.9333333333333333\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "x_train = train.iloc[:,:-1]\n",
    "y_train = train.iloc[:,-1]\n",
    "x_test = test.iloc[:,:-1]\n",
    "y_test = test.iloc[:,-1]\n",
    "\n",
    "linear_kernel = SVC(kernel='linear', C=1).fit(x_train, y_train)\n",
    "print(linear_kernel.score(x_test, y_test))\n",
    "\n",
    "Gaussian_kernel = SVC(kernel='rbf', C= 1).fit(x_train, y_train)\n",
    "print(Gaussian_kernel.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 **[1pt]** Write a Python function that takes two data points and a value for `gamma` as input, and returns the Gaussian kernel of the points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Gaus_kern (x, y, gamma):\n",
    "    return np.exp(-gamma * np.linalg.norm(x-y)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 **[3pt]** Write a Python function that takes two dataset (and a gamma) and returns their Gram matrix for a Gaussian kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You need two datasets because you need to compute the *train* matrix between the train and itself, but the *test* matrix between the test and the train.\n",
    "- Simplest method:\n",
    "    - Create a return matrix, initially empty, shaped size_of_A times size_of_B, with dtype 'float64'\n",
    "    - Run two loops with indices (i, j) in ranges up to size_of_A and size_of_B\n",
    "    - Compute the kernel between row i in A and row j in B, and place it in the return matrix at row i column j\n",
    "- Careful with Pandas' `iterrows()`, as the \"index\" it returns is the DataFrame index (i.e. for use with `loc[]`), not the ordinal index (i.e. for `iloc[`). \n",
    "- Generating the matrix automatically is harder, as there is no straightforward way to compute an `outer` in numpy or pandas with a custom function.\n",
    "- One way is to use `column_stack` https://stackoverflow.com/a/21759340 then apply the kernel defined above.\n",
    "- Another is to use `ufunc.outer` :https://numpy.org/doc/stable/reference/generated/numpy.ufunc.outer.html\n",
    "or http://folk.uio.no/inf3330/scripting/doc/python/NumPy/Numeric/numpy-7.html which is only defined for Universal Functions (`ufuncs`). Look at the examples for `outer`, you can re-implement the function above starting with `np.subtract.outer(A, B)`, which generates the matrix (but check the shape!), then you can run the other operations using broadcast. Both outers and universal functions are super useful, it's worth the effort of learning them, more [[here]](https://docs.scipy.org/doc/numpy/reference/ufuncs.html).\n",
    "- `pandas.apply()` along rows is also an option you should be able to consider with by now. The function name for `-` is `np.subtract` (which is an `ufunc`, see above).\n",
    "\n",
    "Above all, remember the first rule of a good BDD engineer: red, green, refactor! First make it work, then make it better ;) complex solutions are as good as bonus questions here.\n",
    "\n",
    "Also, know that a common default value for gamma is one over the number of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Gram(dset_1, dset_2, gamma= None):\n",
    "    npoints_1, nfeasts_1 = dset_1.shape\n",
    "    npoints_2, nfeasts_2 = dset_2.shape\n",
    "    # number of data points in the two training datasets must be equal\n",
    "    assert nfeasts_1 == nfeasts_2  #always check for consistency\n",
    "    if gamma is None:\n",
    "        gamma = 1/nfeasts_1\n",
    "    \n",
    "    Gram_matrix = np.empty([npoints_1, npoints_2], dtype='float64') #Kernel matrix size n x n\n",
    "    for r in range(npoints_1):\n",
    "        for c in range(npoints_2):\n",
    "            Gram_matrix[r][c] = Gaus_kern(dset_1.iloc[r], dset_2.iloc[c], gamma)\n",
    "    return Gram_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference between inner and outer product can be intuitively summarized as: given two vectors, do you want to aggregate their products into a scalar or distribute the operations over a matrix? The first case is a dot product between e.g. a 1x3 and 3x1 vectors, the second would be between a 3x1 and a 1x3 vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4 **[2pt]** Compute the Gram matrix on the inputs of your datasets. Then train a new SVM, same settings as before with linear kernel, but this time using the Gram matrix('s rows) as the inputs. Print the `score` performance of this new SVM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- With an 80-20 split you are looking at a $120 \\times 120$ shape for the train, and $30 \\times 120$ for the test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120, 120) (30, 120)\n"
     ]
    }
   ],
   "source": [
    "# Compute the Gram matrix on the inputs of datasets (the mappings in destination space)\n",
    "x_train_Gram = Gram(x_train, x_train)\n",
    "x_test_Gram = Gram(x_test, x_train)\n",
    "\n",
    "# check the shapes of the Gram\n",
    "print(x_train_Gram.shape, x_test_Gram.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "# Implement new SVM same setting with linear kernel but using Gram matrix as input\n",
    "\n",
    "linear_kernel_onGram = SVC(kernel='linear').fit(x_train_Gram, y_train)\n",
    "print(linear_kernel_onGram.score(x_test_Gram, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a significant improvement in the score performance of this new SVM. Before is 0.73333, now we get 0.93333"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5 **[1pt]** Plot the confusion matrix for the three SVMs you trained in the past questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's learn a convenient and easy function for this common, very useful metric: `ConfusionMatrixDisplay.from_estimator` [[link here]](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.ConfusionMatrixDisplay.html#sklearn.metrics.ConfusionMatrixDisplay.from_estimator).\n",
    "- So far we saw explicitly 4 cells: true and false positives, true and false negatives. More generally the confusion matrix can be scaled to any number of classes by having the correct labels on the rows, and the predictions on the columns. Errors will be outside the diagonal.\n",
    "- You can use the `normalize` option to get percentages if you like. Which setting do you find most informative?\n",
    "- It's easier if you write a `for` loop over the three models you trained in the previous questions -- just make sure you gave them different names. Also careful as one takes a Gram matrix as input ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'ConfusionMatrixDisplay' has no attribute 'from_estimator'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-5c78b96bd4f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mx_dset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     ConfusionMatrixDisplay.from_estimator(eval(model_name), x_dset, y_test,\n\u001b[0m\u001b[1;32m     17\u001b[0m                                          \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m                                          \u001b[0mdisplay_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclass_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: type object 'ConfusionMatrixDisplay' has no attribute 'from_estimator'"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "class_names = y_test.unique()\n",
    "fig, axes = plt.subplots(1,3)\n",
    "fig.tight_layout()\n",
    "\n",
    "# Here's another trick: you can list the _names_ of the variables, \n",
    "# use their name on the title and fetch their values using `eval()`\n",
    "\n",
    "for model_name, ax in zip([\"lin\", \"Gaus\",\"lin + Gauss\"], axes):\n",
    "    if model_name == \"lin + Gauss\":\n",
    "        x_dset = x_test_Gram\n",
    "    else:\n",
    "        x_dset = x_test\n",
    "    ConfusionMatrixDisplay.from_estimator(eval(model_name), x_dset, y_test,\n",
    "                                         ax = ax,\n",
    "                                         display_labels = class_names,\n",
    "                                         colorbar = False,\n",
    "                                         cmap = plt.cm.Blues,\n",
    "                                         normalize = None)\n",
    "    ax.set_title(model_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
